{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 决策树模型与学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树模型呈树形结构，既可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。学习时，利用训练数据，根据损失函数最小化原则建立决策树模型，预测时，对新的数据，利用决策树模型进行分类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树： 分类决策树模型是一种描述对实例进行分类的树形结构。决策树由结点和有向边组成。结点有两种类型：内部节点和叶结点。内部节点表示一个特征或属性，叶结点表示一个类。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if-then规则：路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "决策树与条件概率分布：决策树表示给定特征条件下类的条件概率分布。将特征空间划分为互不相交的单元，并在每个单元定义一个类的概率分布就构成了一个条件概率分布。决策树的每一条路径对应于划分中的一个单元。各个结点上的条件概率往往偏向于某一个类，即属于某一类的概率较大。决策树分类时将该结点的实例强行分到条件概率大的那一类去。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当损失函数确定后，决策树学习问题就变为在损失函数意义下选择最优决策树的问题。从所有可能的决策树中选取最优决策树是NP完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 特征选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "熵(Entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为：\n",
    "\n",
    "$$P(X=x_i)=p_i,i=1,2,\\dots,n$$\n",
    "\n",
    "随机变量X的熵定义为：\n",
    "\n",
    "$$H(p)=-\\sum_{i=1}^np_i\\log p_i$$\n",
    "\n",
    "对数底为2时，熵的单位称作比特，对数底为e时，熵的单位为纳特。熵只依赖于X的分布，与X的取值无关。熵越大，随机变量的不确定性越大，且满足$0\\leq H(p)\\leq \\log n$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设有随机变量(X，Y),条件熵$H(Y|X)$表示在已知随机变量X的条件下随机变量Y的不确定性。定义为X给定条件下Y的条件概率分布的熵对X的数学期望：\n",
    "\n",
    "$$H(Y|X)=\\sum^n_{i=1}p_iH(Y|X=x_i)$$\n",
    "\n",
    "当熵和条件熵中的概率由数据估计得到时，分别称为经验熵和经验条件熵，如果有0概率，令$0\\log0=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益：表示得知X的信息而使得Y的信息的不确定性减少的程度。\n",
    "\n",
    "特征A对训练数据集D的信息增益$g(D,A)$，定义为集合D的经验熵$H(D)$与特征A给定条件下D的经验条件熵H(D|A)之差，即：\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据信息增益准则的特征选择方法：对训练数据集(或子集)D,计算其每个特征的信息增益，并比较他们的大小，选择信息增益最大的特征。\n",
    "\n",
    "信息增益算法：\n",
    "\n",
    "输入：训练数据集D和特征A\n",
    "\n",
    "输出：特征A对训练数据集D的信息增益$f(D,A)$\n",
    "\n",
    "(1) 计算数据集D的经验熵$H(D)$\n",
    "\n",
    "$$H(D)=-\\sum^K_{k=1}\\frac{|C_k|}{|D|}\\log_2\\frac{|C_k|}{|D}$$\n",
    "\n",
    "(2) 计算特征A对数据集D的经验条件熵$H(D|A)$\n",
    "\n",
    "$$H(D|A)=\\sum_{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\sum_{k=1}{K}\\frac{|D_{ik}|}{|D_i|}\\log_2\\frac{|D_{ik}|}{|D_i|}$$\n",
    "\n",
    "$D_i$为由特征A的不同值将D划分为的子集\n",
    "\n",
    "(3) 计算信息增益\n",
    "\n",
    "$$g(D,A)=H(D)-H(D|A)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "信息增益比：\n",
    "\n",
    "以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题，使用信息增益比可以对这个问题进行校正。\n",
    "\n",
    "特征A对训练数据集D的信息增益比$g_R(D,A)$定义为其信息增益$g(D,A)$与训练数据集D关于特征A的值的熵$H_A(D)$之比，即：\n",
    "\n",
    "$$g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$$\n",
    "\n",
    "其中，$H_A(D)=-\\sum_{i=1}^n\\frac{|D_i|}{|D|}\\log_2\\frac{|D_i|}{|D|}$,n是特征A取值的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 决策树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。C4.5算法在生成的过程中，用信息增益比来选择特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入：训练数据集D,特征集A,阈值\n",
    "\n",
    "输出：决策树T\n",
    "\n",
    "(1) 若D中所有实例属于同一类$D_k$,则T为单结点树，并将类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "(2) 若A为空集，则T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "(3) 否则，计算A中各特征对D的信息增益/信息增益比，选择信息增益/信息增益比最大的特征$A_g$\n",
    "\n",
    "(4) 如果$A_g$的信息增益小于阈值，则置T为单结点树，并将D中实例数最大的类$C_k$作为该结点的类标记，返回T\n",
    "\n",
    "(5) 否则，对$A_g$的每一个可能值$a_i$,依$A_g=a_i$将D分割成若干个非空子集$D_i$,将$D_i$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成数T，返回T\n",
    "\n",
    "(6) 对第i个子结点，以$D_i$为训练集，以$A-{A_g}$为特征集，递归的调用(1)-(5)，得到子树T，返回T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 决策树的剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "采用递归方式生成的决策树，由于在学习时过多的考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树，导致过拟合。在决策树学习中将已经生成的树进行简化的过程称为剪枝，具体的，剪枝从已生成的树上裁减掉一些子树或者叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设树T的叶结点个数为|T|,t是树T的叶结点，该叶结点有$N_t$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,\\dots,K$,$H_t(T)$为叶结点t上的经验熵，$\\alpha$为平衡因子，决策树学习的损失函数可以定义为：\n",
    "\n",
    "$$C_{\\alpha}(T)=\\sum_{t=1}^{|T|}N_tH_t(T)+\\alpha|T|$$\n",
    "\n",
    "其中经验熵为\n",
    "\n",
    "$$H_t(T)=-\\sum_k\\frac{N_{tk}}{N_t}\\log\\frac{N_{tk}}{N_t}$$\n",
    "\n",
    "这时有：\n",
    "\n",
    "$$C_{\\alpha}(T)=C(T)+\\alpha|T|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该损失函数同时考虑了模型对训练数据的拟合程度与模型的复杂度，通过最小化该损失函数可以对模型进行剪枝，剪枝算法可通过动态规划算法实现：\n",
    "\n",
    "输入：生成算法产生的整个树T,参数$\\alpha$\n",
    "\n",
    "输出：修剪后的自述$T_{\\alpha}$\n",
    "\n",
    "(1) 计算每个结点的经验熵\n",
    "\n",
    "(2) 递归的从树的叶结点向上回缩，设一组叶结点回缩到期父结点之前与之后的整体树分别为$T_B$与$T_A$,其对应的损失函数值分别是$C_{\\alpha}(T_B)$与$C_{\\alpha}(T_A)$，如果：\n",
    "\n",
    "$$C_{\\alpha}(T_A)\\leq C_{\\alpha}(T_B)$$\n",
    "\n",
    "则进行剪枝，将父结点变为新的叶结点\n",
    "\n",
    "(3) 返回(2),直至不能继续为止，得到损失函数最小的自述$T_{\\alpha}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CART算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分类与回归树(classification and regression tree, CART)。CART假设决策树是二叉树，内部结点特征的取值为\"是\"和\"否\"。决策树的生成就是递归的构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 最小回归树生成算法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入： 训练数据集D\n",
    "\n",
    "输出：回归树$f(x)$\n",
    "\n",
    "在训练数据集所在的输入空间中，递归的将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树\n",
    "\n",
    "(1) 选择最优切分变量j与切分点s，求解\n",
    "\n",
    "$$\\min_{j,s}[\\min_{c_!}\\sum_{x_i\\in R_1(j,s)}(y_i-c_1)^2+\\min_{c_2}\\sum_{x_i\\in R_2(j,s)}(y_i-c_2)^2]$$\n",
    "\n",
    "遍历变量j,对固定的切分变量j扫描切分点s，选择使上式达到最小值是的对(j,s)\n",
    "\n",
    "(2) 用选定的对(j,s)划分区域并决定相应的输出值，输出值为该区域内所有样本点输出值的平均值\n",
    "\n",
    "(3) 继续对两个子区域调用步骤(1),(2)，直至满足停止条件\n",
    "\n",
    "(4) 将输入空间划分为M个区域，生成决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 分类树的生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基尼指数：分类问题中，假设有K个类，样本点属于第k个类的概率为$p_k$，则概率分布的基尼指数定义为：\n",
    "\n",
    "$$Gini(p)=\\sum_{k=1}^Kp_k(1-p_k)=1-\\sum_{k=1}^Kp^2_k$$\n",
    "\n",
    "对于一个给定的样本集合D,其基尼指数为:\n",
    "\n",
    "$$Gini(D)=1-\\sum_{k=1}^K(\\frac{|C_k|}{|D|})^2$$\n",
    "\n",
    "如果样本结合D因为特征A是否取某一个可能值a被分成$D_1$和$D_2$两部分，则在特征A的条件下，集合D的基尼指数定义为：\n",
    "\n",
    "$$Gini(D,A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$$\n",
    "\n",
    "基尼指数越大，样本集合的不确定性越大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART生成算法：\n",
    "\n",
    "输入：训练数据集D,停止计算的条件\n",
    "\n",
    "输出：CART树\n",
    "\n",
    "根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：\n",
    "\n",
    "(1) 设结点的训练数据集为D,计算现有特征对该数据集的基尼指数。此时，对每一个特征A,对其可能取的每个值a,根据样本点对$A=a$的测试为“是”或“否”将D分为两个部分，计算$A=a$时的基尼指数。\n",
    "\n",
    "(2) 在所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应的切分点作为最优特征和最优切分点。依最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。\n",
    "\n",
    "(3) 对两个子结点递归地调用(1),(2),直至满足停止条件\n",
    "\n",
    "(4) 生成CART决策树\n",
    "\n",
    "算法停止计算的条件是结点中的样本个数小于预订阈值，或样本集的基尼指数小于约定阈值，或者没有更多特征。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CART树剪枝：\n",
    "\n",
    "对于任意一个子树T,其损失函数为:\n",
    "\n",
    "$$C_{\\alpha}(T)=C(T)+\\alpha|T|$$\n",
    "\n",
    "$C(T)$为测试误差（如基尼指数），$|T|$为子树的叶结点个数。其计算的是对训练数据的拟合程度与模型复杂度之间的权衡。对于固定的$\\alpha$,一定存在使损失函数最小的一棵子树，且是唯一存在的。在$\\alpha$的不同取值空间上，最优子树是不同的。\n",
    "\n",
    "对于整体树的内部节点t，以t为单结点树的损失函数为:\n",
    "\n",
    "$$C_{\\alpha}(t)=C(t)+\\alpha$$\n",
    "\n",
    "以t为根结点的子树$T_t$的损失函数是：\n",
    "\n",
    "$$C_{\\alpha}(T_t)=C(T_t)+\\alpha|T_t|$$\n",
    "\n",
    "令$C_{\\alpha}(t)=C_{\\alpha}(T_t)$可以得到:\n",
    "\n",
    "$$\\alpha=\\frac{C(t)-C(T_t)}{|T_t|-1}$$\n",
    "\n",
    "当$\\alpha$小于这个值时，保留子树误差更小，当$\\alpha$大于这个值时，剪枝后预测误差更小。不断增加$\\alpha$的值，就可以得到剪枝后的子树序列，$T_0,T_1,\\dots,T_n$，且这些子树是嵌套的，每棵子树都有一个对应的参数$\\alpha$。\n",
    "\n",
    "利用独立的验证数据集，测试子树序列中各个子树的平方误差或基尼指数，平方误差或基尼指数最小的决策树被认为是最优的决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "输入: CART算法生成的决策树$T_0$\n",
    "\n",
    "输出:最优决策树$T_{\\alpha}$\n",
    "\n",
    "(1) 设$k=0,T=T_0$\n",
    "\n",
    "(2) 设$\\alpha=+\\infty$\n",
    "\n",
    "(3) 自下而上地对内部结点 $t$ 计算$C(T_t),|T_t|$，以及：\n",
    "\n",
    "$$g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$$\n",
    "\n",
    "这里需要遍历所有的内部结点，完成计算，然后选择一个最小的$g(t)$作为$\\alpha$\n",
    "\n",
    "(4) 对$g(t)=\\alpha$的内部节点t进行剪枝，并对叶结点t以多数表决法决定其类，得到树T\n",
    "\n",
    "(5) 设$k=k+1,\\alpha_k=\\alpha,T_k=T$\n",
    "\n",
    "(6) 如果$T_k$不是由根结点及两个叶结点构成的树，则回到步骤(2)，否则令$T_k=T_n$,不再继续剪枝。\n",
    "\n",
    "(7) 采用交叉验证法在子树序列$T_0,T_1,\\dots,T_n$中选取最优子树$T_{\\alpha}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
